{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Paths to files\n",
        "dictionary_path = '/content/drive/MyDrive/AI project/extended_sinhala_dictionary.txt'\n",
        "sentences_path = '/content/drive/MyDrive/AI project/sentences.txt'\n",
        "\n",
        "# Load dictionary\n",
        "def load_dictionary(dictionary_path):\n",
        "    with open(dictionary_path, 'r', encoding='utf-8') as file:\n",
        "        return set(word.strip() for word in file)\n",
        "\n",
        "# Update dictionary with words from sentences file\n",
        "def update_dictionary(dictionary_path, sentences_path):\n",
        "    dictionary = load_dictionary(dictionary_path)\n",
        "\n",
        "    # Extract words from sentences\n",
        "    with open(sentences_path, 'r', encoding='utf-8') as file:\n",
        "        sentences = file.readlines()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.strip().split()\n",
        "        dictionary.update(words)\n",
        "\n",
        "    # Save updated dictionary\n",
        "    with open(dictionary_path, 'w', encoding='utf-8') as file:\n",
        "        file.write('\\n'.join(sorted(dictionary)))\n",
        "\n",
        "# Initialize dictionary\n",
        "update_dictionary(dictionary_path, sentences_path)"
      ],
      "metadata": {
        "id": "YxiBytt8yngg"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Paths to files\n",
        "dictionary_path = '/content/drive/MyDrive/AI project/extended_sinhala_dictionary.txt'\n",
        "sentences_path = '/content/drive/MyDrive/AI project/sentences.txt'\n",
        "\n",
        "# Load dictionary\n",
        "def load_dictionary(dictionary_path):\n",
        "    with open(dictionary_path, 'r', encoding='utf-8') as file:\n",
        "        return set(word.strip() for word in file)\n",
        "\n",
        "# Update dictionary with words from sentences file\n",
        "def update_dictionary(dictionary_path, sentences_path):\n",
        "    dictionary = load_dictionary(dictionary_path)\n",
        "\n",
        "    # Extract words from sentences\n",
        "    with open(sentences_path, 'r', encoding='utf-8') as file:\n",
        "        sentences = file.readlines()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.strip().split()\n",
        "        dictionary.update(words)\n",
        "\n",
        "    # Save updated dictionary\n",
        "    with open(dictionary_path, 'w', encoding='utf-8') as file:\n",
        "        file.write('\\n'.join(sorted(dictionary)))\n",
        "\n",
        "# Initialize dictionary\n",
        "update_dictionary(dictionary_path, sentences_path)\n",
        "\n",
        "import re\n",
        "from difflib import get_close_matches\n",
        "\n",
        "# Spell checker\n",
        "def spell_checker(text, dictionary):\n",
        "    words = text.split()\n",
        "    errors = {}\n",
        "    for word in words:\n",
        "        if word not in dictionary:\n",
        "            suggestions = get_close_matches(word, dictionary, n=3)\n",
        "            errors[word] = suggestions\n",
        "    return errors\n",
        "\n",
        "# Grammar checker (basic example: contextual grammar issues)\n",
        "def grammar_checker(text):\n",
        "    grammar_errors = []\n",
        "    # Example: incorrect use of duplicate words\n",
        "    duplicate_pattern = re.compile(r'\\b(\\w+) \\1\\b')\n",
        "    for match in duplicate_pattern.finditer(text):\n",
        "        grammar_errors.append((match.group(), match.start(), match.end()))\n",
        "    return grammar_errors\n",
        "\n",
        "# Highlight errors\n",
        "def highlight_errors(text, spell_errors, grammar_errors):\n",
        "    for word, suggestions in spell_errors.items():\n",
        "        text = text.replace(word, f'<span style=\"color:red;\">{word}</span>')\n",
        "    for error, start, end in grammar_errors:\n",
        "        text = text[:start] + f'<span style=\"color:red;\">{error}</span>' + text[end:]\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "dictionary = load_dictionary(dictionary_path)\n",
        "input_text = \"අපි හිරු උදාව බලා සිටිමු. අපි කාර්යබහුල වීදිය හරහා ගමන් කළෙමු. ළමයි දවසට සත්තු වත්තට ගියහ. මම මගේ මිතුරන් සමඟ පැසිපන්දු ක්‍රීඩා කලෙමි. ඇය නවකතාවේ පරිච්ඡේදයක් කියෙව්වාය. මම මනස සැහැල්ලු කර ගැනීමට යෝග ක්‍රියාකාරකම් කලෙමි. ඔවුහු සැමරුමට එක් වූහ.\"\n",
        "spell_errors = spell_checker(input_text, dictionary)\n",
        "grammar_errors = grammar_checker(input_text)\n",
        "corrected_text = highlight_errors(input_text, spell_errors, grammar_errors)\n",
        "\n",
        "print(\"Spell Errors:\", spell_errors)\n",
        "print(\"Grammar Errors:\", grammar_errors)\n",
        "print(\"Corrected Text:\", corrected_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C3IuzXYylXk",
        "outputId": "7c76e6be-ec7a-4167-d6f0-1cd2b7154871"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spell Errors: {}\n",
            "Grammar Errors: []\n",
            "Corrected Text: අපි හිරු උදාව බලා සිටිමු. අපි කාර්යබහුල වීදිය හරහා ගමන් කළෙමු. ළමයි දවසට සත්තු වත්තට ගියහ. මම මගේ මිතුරන් සමඟ පැසිපන්දු ක්‍රීඩා කලෙමි. ඇය නවකතාවේ පරිච්ඡේදයක් කියෙව්වාය. මම මනස සැහැල්ලු කර ගැනීමට යෝග ක්‍රියාකාරකම් කලෙමි. ඔවුහු සැමරුමට එක් වූහ.\n"
          ]
        }
      ]
    }
  ]
}