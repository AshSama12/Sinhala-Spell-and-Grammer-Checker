{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxXKh/WHLFTRWdyBK+iQvu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshSama12/Sinhala-Spell-and-Grammer-Checker/blob/master/Untitled43.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T0CsdWeSwCsJ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# List of text files with your dataset\n",
        "text_files = [\n",
        "    '/content/business.txt',\n",
        "    '/content/environmental.txt',\n",
        "    '/content/history.txt',\n",
        "    '/content/korea.txt',\n",
        "    '/content/latest.txt',\n",
        "    '/content/medical.txt',\n",
        "    '/content/medical2.txt',\n",
        "    '/content/medical3.txt',\n",
        "    '/content/newspaper1.txt',\n",
        "    '/content/newspaper2.txt',\n",
        "    '/content/newspaper3.txt',\n",
        "    '/content/newspaper4.txt',\n",
        "    '/content/novel1.txt',\n",
        "    '/content/novel2.txt',\n",
        "    '/content/novel3.txt',\n",
        "    '/content/novel4.txt',\n",
        "    '/content/novel5.txt',\n",
        "    '/content/novel6.txt',\n",
        "    '/content/political.txt',\n",
        "    '/content/social.txt',\n",
        "    '/content/adivasi.txt',\n",
        "    '/content/buddhist.txt',\n",
        "    '/content/canada.txt',\n",
        "    '/content/janakavi.txt',\n",
        "    '/content/india.txt',\n",
        "    '/content/mahinda.txt',\n",
        "    '/content/srilanka.txt',\n",
        "    '/content/gotabhaya.txt',\n",
        "    '/content/folk story.txt',\n",
        "    '/content/os.txt',\n",
        "    '/content/computer.txt'\n",
        "\n",
        "]\n",
        "\n",
        "# Function to extract Sinhala words using Unicode range\n",
        "def extract_sinhala_words(text):\n",
        "    # Regular expression to match Sinhala words (Unicode range for Sinhala)\n",
        "    sinhala_words = re.findall(r'[\\u0D80-\\u0DFF]+', text)\n",
        "    return sinhala_words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store unique Sinhala words\n",
        "sinhala_word_set = set()\n",
        "\n",
        "# Loop over all the text files\n",
        "for file_path in text_files:\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "            # Extract Sinhala words from the text\n",
        "            words = extract_sinhala_words(text)\n",
        "            # Update the set with unique words\n",
        "            sinhala_word_set.update(words)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")"
      ],
      "metadata": {
        "id": "hUk-hnSj2Lag"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path where the dictionary will be saved\n",
        "dictionary_path = '/content/sinhala_dictionary.txt'\n",
        "\n",
        "# Write the unique Sinhala words to the dictionary file\n",
        "with open(dictionary_path, 'w', encoding='utf-8') as dict_file:\n",
        "    for word in sorted(sinhala_word_set):\n",
        "        dict_file.write(word + '\\n')\n",
        "\n",
        "print(f\"Dictionary created with {len(sinhala_word_set)} unique Sinhala words.\")\n",
        "print(f\"Dictionary saved at {dictionary_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF50J5jy27Bk",
        "outputId": "5049ec58-b49f-4e07-dc8c-c111402f63ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary created with 13689 unique Sinhala words.\n",
            "Dictionary saved at /content/sinhala_dictionary.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dictionary into a set\n",
        "dictionary_path = '/content/sinhala_dictionary.txt'\n",
        "with open(dictionary_path, 'r', encoding='utf-8') as f:\n",
        "    sinhala_dictionary = set(f.read().splitlines())\n",
        "\n",
        "print(f\"Loaded {len(sinhala_dictionary)} words into the dictionary.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVxMwo3WS4H9",
        "outputId": "1b1d5d92-ed5d-4770-cb3a-98085400a915"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 13689 words into the dictionary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to tokenize text into words\n",
        "def tokenize_text(text):\n",
        "    # Match Sinhala words using Unicode range and split by spaces/punctuation\n",
        "    words = re.findall(r'[\\u0D80-\\u0DFF]+', text)\n",
        "    return words\n",
        "\n",
        "# Example input\n",
        "input_text = \"අක්ක ඉගෙන ගන්න ගියා.\"\n",
        "tokens = tokenize_text(input_text)\n",
        "print(\"Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpaenXEFS67k",
        "outputId": "04a236df-c9f0-4b14-acf4-56fc7b09f70a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['අක්ක', 'ඉගෙන', 'ගන්න', 'ගියා']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check for misspellings\n",
        "def check_spelling(tokens, dictionary):\n",
        "    misspelled_words = []\n",
        "    for word in tokens:\n",
        "        if word not in dictionary:\n",
        "            misspelled_words.append(word)\n",
        "    return misspelled_words\n",
        "\n",
        "# Check for misspellings\n",
        "misspelled = check_spelling(tokens, sinhala_dictionary)\n",
        "print(\"Misspelled words:\", misspelled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keKC2krkTHbv",
        "outputId": "26cff4b3-cdd5-4c78-876a-ec95079c5306"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Misspelled words: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rapidfuzz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVSTVEmsTWIb",
        "outputId": "0b56fc5b-0fc9-4aaf-db54-714445bfc3d4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rapidfuzz import process\n",
        "\n",
        "# Function to suggest corrections\n",
        "def suggest_corrections(word, dictionary, limit=3):\n",
        "    # Find the closest matches in the dictionary\n",
        "    suggestions = process.extract(word, dictionary, limit=limit)\n",
        "    return [match[0] for match in suggestions]\n",
        "\n",
        "# Example: Suggest corrections for misspelled words\n",
        "for word in misspelled:\n",
        "    suggestions = suggest_corrections(word, sinhala_dictionary)\n",
        "    print(f\"Suggestions for '{word}': {suggestions}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTVZNtZITNAY",
        "outputId": "f6110d18-e374-4703-a3a4-e8b2e8c1e30e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suggestions for 'පරාසයකි': ['පරාසයක', 'ා', 'පර']\n",
            "Suggestions for 'සින්හලය': ['හ', 'න', 'සින්']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to correct text\n",
        "def correct_text(input_text, dictionary):\n",
        "    tokens = tokenize_text(input_text)\n",
        "    corrected_text = input_text\n",
        "    for word in tokens:\n",
        "        if word not in dictionary:\n",
        "            suggestions = suggest_corrections(word, dictionary)\n",
        "            if suggestions:\n",
        "                # Automatically replace with the best suggestion\n",
        "                corrected_word = suggestions[0]\n",
        "                print(f\"Correcting '{word}' to '{corrected_word}'\")\n",
        "                corrected_text = corrected_text.replace(word, corrected_word)\n",
        "    return corrected_text\n",
        "\n",
        "# Correct the example text\n",
        "corrected = correct_text(input_text, sinhala_dictionary)\n",
        "print(\"Corrected text:\", corrected)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJRoAm6NTftu",
        "outputId": "dcf41255-289b-4ff3-9153-87574f337911"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correcting 'පරාසයකි' to 'පරාසයක'\n",
            "Correcting 'සින්හලය' to 'හ'\n",
            "Corrected text: මෙය එක උදාහරණ පරාසයක. හ හොඳට ඉගෙන ගන්න.\n"
          ]
        }
      ]
    }
  ]
}